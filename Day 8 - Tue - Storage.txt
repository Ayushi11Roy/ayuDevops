Storage:
	SAN (Storage Area Network) -> in terms of Amazon, we call it EBS (Elastic Block Store) :- 
		1. SSD : gp2, gp3, io1, io2  (GP: General purpose)
		2. HDD : sc-1, st-1
		3. Tape Drive: Cached
		4. Ephemeral (Temporary) Storage
	NAS ->
		1. HTTPD, FFS, NFS, CFS, Samba
	Object -> AWS S3

- OS should be stored inside EBS.
- Why is always EBS volume recommended for file system?
- GP2 min partition is 1 GB, max is 16 TB
- IO1 and IO2 are too expensive storage. Large operations like read/write are carried on using this.
- EBS will not attach simultaneously with multiple server. It will not be replicated from one region to another region/zone
- Major advantage of cloud storage is that we can maintain our disc performance.
- Cloud gives us IOPS (input/output processing per second)

LVM - Logical Volume Manager

- Hard disk (HDD) is ideal for backup. 

EFS : Elastic File Server

- Here, we're sharing file system, so simultaneously multiple people can read/write.
- cannot split fs. Cannot make partitions.

EBS is volume. We can make partitions.

Shared File Access: Unlike services like Amazon Elastic Block Store (EBS) which are tied to a single instance, EFS allows multiple Amazon EC2 instances, containers (Amazon ECS, Amazon EKS), serverless functions (AWS Lambda), and even on-premise servers to access the same data simultaneously. This is ideal for scenarios where applications need to share common files or data sets.

In EFS, we can share within the same region but across multiple zones.

Sharing within the same region (across multiple AZs): This is a core feature of Amazon EFS. You can create an EFS file system within a Virtual Private Cloud (VPC), and then attach or mount it to multiple Amazon EC2 instances running in different AZs within that same AWS Region. EFS ensures that data is stored redundantly across multiple AZs within the region for high availability and durability. You'll create mount targets in the relevant AZs within the VPC, allowing instances in each AZ to access the shared file system. For optimal performance and cost, it's recommended to access the file system from a mount target within the same Availability Zone as your EC2 instance.



======================
LAB:

How to increase root-volume:


cat /etc/os-release
yum install httpd -y
cd /var/www/html/
cat > index.html
Hello!
ll
systemctl start httpd
systemctl enable httpd
cd
curl http://localhost
df -h
[Root partition is /dev/xvda1]
[Say it's not sufficient, so we can extend the root volume by attaching volume]

EC2 -> Volume -> Rename the vol as root-vol -> Actions -> Modify Volume -> gp2 and 20G

df -h  (Now, vol added will not show yet, but physically it was resized.)
blkid
lsblk
df -h
growpart /dev/xvda
growpart /dev/xvda 1  (Here, 1 is the partition no. To help resize, ie, increase xfs partition. Xfs partition only increases, but ext4 or ext3 can increase as well as decrease)
xfs_growfs -d /
df -h

----------------------------

Partition data-volume (the one we are attaching to instance. Root-vol can't be partitioned.) Partition of 2 types: Primary and extended)

Partitions:

We can make no of partitions according to disk.
MS dos part, scheame (doesn't support more than 2TB disk)
GPT (replaced MS dos)


-----------------
Create a vol -> attach to instance ->

fdisk -l
fdisk /dev/xvdb
m
n (add a new partition)
e
4
{Enter}
{Enter}
w

partprobe
fdisk -l

fdisk /dev/xvdb
n
<Enter>
+3G
w



fdisk /dev/xvdb
n
l
{enter}
<Enter>
w

===========================================

You can also build the extended partitions like this:

fdisk /dev/xvda  (go to the target DISK)
p  (print to verify)
n  (new partition creation)
e  (extended partition type)
4  (partition number)
<Enter>
<Enter> (Accepting the default 1st and last sectors: This will create extended partition across the available space)
(Do not write, ie, 'w' yet!!)

{Now, creating logical partitions within the extended partition}

n (Initiating new partition for 1st logical partition)
<Enter>
+3G

{Repeat the process for the following partitions you want to create}

n
<Enter>
<Enter>  (Pressing enter to take the remaining +2G space)
p        (Verify)
w        (Write)

partprobe 

===========================================

fdisk -l
mkfs.ext4 /dev/xvdb5    (Creating filesystems)
mkfs.ext4 /dev/xvdb6
blkid

mkdir /devops
mount /dev/xvdb5 /devops
mkdir /data
mount /data/xvdb6 /data

cd /devops
touch test.txt{1..100}
ls

For permanent mounting, we can go to /etc/fstab and paste the uuid.

REMOVING THE VOL:

cd
umount /devops
umount /data

In aws:

EC2 -> vol -> action -> detach -> check <lsblk> (vol should be gone)
vol -> action -> delete

We can't delete vol without unmounting, because system/instance will crash.



-------------------------------------------------


Create a new volume (5G and all that) -> give name -> attach to instance

lsblk
(and all the commands we run on terminal to mount the vol)
Then, go the folder you created. Create empty files using touch.
We need to replicate the files. (Cross-region, zone-zone, one zone-another zone)

Cross-region:

AWS N.Virginia & Ohio

[We do it using snapshot. By default, it'll automatically inherit all the zones in the region.]

vol-created -> actions -> create snapshot
We can see snapshots under EBS -> snapshots. (Snapshot is created of the whole volume, and not of the data.)

Snapshot is stored inside S3.
By default, S3 will automatically replicate data in 3 or more than 3 region.
Snapshot is a region-based service.

Snapshot -> action -> copy snapshot -> select region as ohio/us-east-2.
To see the copied snapshot, we go the region we selected -> snapshot -> we can see there


Launch an instance from Ohio (the region you created the snapshot).

After instance is created. Snapshots -> actions -> create volume from snapshot.
Select volume -> attach volume -> go to terminal

lsblk
lsblk -fs
blkid (we can see the file-system as well. Do not create new file-system, since snapshot captures that as well)
mount
mkdir test
touch file.txt{1..100}
ls
unmount

[if we change any data, it'll not be shown in the snapshot. For that, we need to create new snapshot, and copy.]


We can add snapshot to different account as well.

-----------------------------

SNAPSHOT:

Create snapshot from volume.
Then, select the snapshot by going to snapshots, then select "Create volume from snapshot" 
There, we can see availability zone, and select same region but different zone. Same zone also same.
For different region, we need to do copy snapshot.

(-- why we do this? What do we accomplish?)

----------------------------------

EFS (ELASTIC FILE SHARE):

NFS : Network File Share
SMB Protocol: System Message Block

Centralized sharing


1. Launch instances (with different OS : 1 redhat, 1 amazon, 1 ubuntu - 3 instances total)
2. Make 3 different zones for the 3 instances.
3. EFS -> Create file system
4. name -> vpc (default) -> create
5. Now we need to connect the file-systems with the instances.

- connect the instances to the terminal using usual commands
- rpmquery nfs-utils
- systemctl start nfs-server.service
- systemctl status nfs-server.service

mkdir data ->error


For redhat machine:

- ssh -i ....... and sudo su - and all that
- yum install nfs-utils -y
- systemctl start nfs-utils.service

For ubuntu:

- sudo su -
- apt update
- apt install nfs-common
- systemctl start nfs-utils.service
- 


----

efs id -> attach -> copy the "sudo mount -t command
Need to crate a nfs security group.
FS -> Manage

Replication process EFS
(CRR: Cross Region Replication)